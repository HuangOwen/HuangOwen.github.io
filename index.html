<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xijie Huang</title>
  
  <meta name="author" content="Xijie Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xijie Huang (Owen) ÈªÑÊÇâÂÅà</name>
              </p>
              <p>I am a PhD Student at <a href="https://hkust.edu.hk/">Hong Kong University of Science and Technology (HKUST)</a>, where I work on efficient deep learning (algorithm-hardware co-design) and computer vision. I am a member of <a href="http://vsdl.ust.hk/index.html">Vision and System Design Lab (VSDL)</a>, advised by <a href="https://seng.ust.hk/about/people/faculty/tim-kwang-ting-cheng?id=326">Prof. Tim Kwang-Ting CHENG</a> and <a href="http://zhiqiangshen.com/">Prof. Zhiqiang Shen</a>. Previously, I work with <a href="https://www.mvig.org/">Prof.Cewu Lu</a> from SJTU and <a href="https://www.ee.ucla.edu/mani-srivastava/">Prof.Mani Srivastava</a> from UCLA. 
              </p>
              </p>
              <p style="text-align:center">
                <a href="files/CV_for_XijieHuang.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=nFW2mqwAAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/Owen_Huangxj">Twitter</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/huang-xi-jie-17">Zhihu (Áü•‰πé)</a> &nbsp/&nbsp
                <a href="https://github.com/HuangOwen">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images_all/my_photo1.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images_all/my_photo1.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests lie in the general area of artificial intelligence, particularly in model compression and its applications in computer vision. More concretely, My research interests focus on quantization, pruning, algorithm-hardware co-design, and human-centric computer vision. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/SDQ.png', height="140" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>SDQ: Stochastic Differentiable Quantization with Mixed Precision</papertitle>
              <br>
              <b>Xijie Huang</b>, Zhiqiang Shen, Shichao Li, Zechun Liu, Xianghong Hu, Jeffry Wicaksana, Eric Xing, Kwang-Ting Cheng
              <br>
              <em>ICML</em>, 2022 &nbsp <font color="red"><strong>(Spotlight)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2206.04459">Paper</a> /
              <a href="https://www.bilibili.com/video/BV1uY4y1E7Nk/?share_source=copy_web&vd_source=9b0b25da7812c022f1ee27fdc58aeb82">Talk</a> / 
              <a href="https://icml.cc/media/icml-2022/Slides/16258.pdf">Slides</a> 
              <p>A novel stochastic quantization framework to learn the optimal mixed precision quantization strategy. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/EVQ.png', height="130" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Variation-aware Vision Transformer Quantization</papertitle>
              <br>
              <b>Xijie Huang</b>, Zhiqiang Shen, Kwang-Ting Cheng
              <br>
              <em>In Submission</em>
              <br>
              <a href="https://arxiv.org/abs/2307.00331">Paper</a>/<a href="https://github.com/HuangOwen/VVTQ">Code</a>
              <p></p>
              <p>An analysis of the underlying difficulty of ViT quantization in the view of variation. A multi-crop knowledge distillation-based quantization method is proposed.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/QAT-ACS.png', height="120" width="190"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Efficient Quantization-aware Training with Adaptive Coreset Selection</papertitle>
              <br>
              <b>Xijie Huang</b>, Zechun Liu, Shih-Yang Liu, Kwang-Ting Cheng
              <br>
              <em>In Submission</em>
              <br>
              <a href="https://arxiv.org/abs/2306.07215">Paper</a>/<a href="https://github.com/HuangOwen/QAT-ACS">Code</a>
              <p></p>
              <p>A new angle through the coreset selection to improve the training efficiency of quantization-aware training. Our method can achieve an accuracy of 68.39% of 4-bit quantized ResNet-18 on the ImageNet-1K dataset with only a 10% subset, which has an absolute gain of 4.24% compared to the previous SoTA.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/HOHC.png', height="140" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Automated Vision-Based Wellness Analysis for Elderly Care Centers</papertitle>
              <br>
              <b>Xijie Huang</b>, Jeffry Wicaksana, Shichao Li, Kwang-Ting Cheng
              <br>
              <em>AAAI Workshop on Health Intelligence, 2022</em>
              <br>
              <a href="https://arxiv.org/abs/2112.10381">Paper</a>
              <p></p>
              <p>An automatic, vision-based system for monitoring and analyzing the physical and mental well-being of senior citizens.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/TIN.png', height="140" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Transferable Interactiveness Prior for Human-Object Interaction Detection</papertitle>
              <br>
              Yong-Lu Li, Siyuan Zhou, <b>Xijie Huang</b>, Liang Xu, Ze Ma, Hao-Shu Fang, Cewu Lu
              <br>
              <em>TPAMI 2021/CVPR 2019</em>
              <br>
              <a href="https://arxiv.org/abs/2101.10292">Paper (TPAMI version)</a> /
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Transferable_Interactiveness_Knowledge_for_Human-Object_Interaction_Detection_CVPR_2019_paper">Paper (CVPR version)</a> /
              <a href="https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network">Code</a> 
              <p></p>
              <p>A transferable knowledge learner and can be cooperated with any HOI detection models to achieve desirable results. TIN outperforms state-of-the-art HOI detection results by a great margin, verifying its efficacy and flexibility.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/pasta.png', height="120" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>PaStaNet: Toward Human Activity Knowledge Engine</papertitle>
              <br>
              Yong-Lu Li, Liang Xu, Xinpeng Liu, <b>Xijie Huang</b>, Mingyang Chen, Shiyi Wang, Hao-Shu Fang, Cewu Lu
              <br>
              <em>CVPR 2020</em>
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Li_PaStaNet_Toward_Human_Activity_Knowledge_Engine_CVPR_2020_paper.html">Paper</a>/
              <a href="https://github.com/DirtyHarryLYL/HAKE-Action">Code</a>
              <p></p>
              <p>A large-scale knowledge base PaStaNet, which contains 7M+ PaSta annotations. And two corresponding models are proposed: first, we design a model named Activity2Vec to extract PaSta features, which aim to be general representations for various activities.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/finger.png', height="120" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Latent Fingerprint Image Enhancement based on progressive generative adversarial network</papertitle>
              <br>
              <b>Xijie Huang</b>, Peng Qian, Manhua Liu
              <br>
              <em>CVPRW 2020</em>
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w48/Huang_Latent_Fingerprint_Image_Enhancement_Based_on_Progressive_Generative_Adversarial_Network_CVPRW_2020_paper.html">Paper</a> 
              <p></p>
              <p>A latent fingerprint enhancement method based on the progressive generative adversarial network (GAN).
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/HAKE.png', height="120" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>HAKE: Human Activity Knowledge Engine</papertitle>
              <br>
              Yong-Lu Li, Liang Xu, Xinpeng Liu, <b>Xijie Huang</b>, Ze Ma, Hao-Shu Fang, Cewu Lu
              <br>
              <em>Preprint</em>
              <br>
              <a href="https://arxiv.org/abs/1904.06539">Paper</a>/
              <a href="http://hake-mvig.cn/home/">Project webpage</a>
              <p></p>
              <p>A large-scale Human Activity Knowledge Engine (HAKE) based on the human body part states to promote the activity understanding.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/trojan.png', height="120" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>NeuronInspect: Detecting Trojan Backdoors in Deep Neural Networks via VisualInterpretability</papertitle>
              <br>
              <b>Xijie Huang</b>, Moustafa Alzantot, Mani.Srivastava
              <br>
              <em>Preprint</em>
              <br>
              <a href="https://arxiv.org/pdf/1911.07399.pdf">Paper</a>
              <p></p>
              <p>A framework to detect trojan backdoors in DNNs via output explanation techniques
              </p>
            </td>
          </tr>



        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              Reviewer, NeurIPS 2023, ICCV 2023, CVPR 2023, WACV 2022, AAAI 2022, ECCV 2022
              <br>
              <strong>Top 10% Reviewer</strong>, ICML 2022
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TA.png" , height="123" width="160", alt="cs188">
            </td>
            <td width="75%" valign="center">
              COMP 2211 (Exploring Artificial Intelligence), Lecture: Professor Desmond Tsoi
              <br>
              COMP 5421 (Computer Vision), Lecture: Professor Dan Xu
              <br>
              COMP 1021 (Introduction to Computer Science), Lecturer: Professor David Rossitor
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/scholarship.png" , height="123" width="160", alt="cs188">
            </td>
            <td width="75%" valign="center">
              National Scholarship (Top 2% students in SJTU), 2017
              <br>
              A Class Scholarship (Top 2% students in SJTU), 2017
              <br>
              CSST Scholarship (USD $5,343 from UCLA), 2019
              <br>
              RongChang Academic Scholarship (Top 20 in SJTU), 2019
              <br>
              RedBird Scholarship, Postgraduate Studentship, 2020-2022
              <br>
              AAAI-22 Student Scholarship, 2022
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Thanks to <a href="https://github.com/jonbarron/jonbarron_website">Barron's website template </a>. 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
