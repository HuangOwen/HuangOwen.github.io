<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xijie Huang</title>
  
  <meta name="author" content="Xijie Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xijie Huang (Owen) ÈªÑÊÇâÂÅà</name>
              </p>
              <p>I am a PhD Student at <a href="https://hkust.edu.hk/">Hong Kong University of Science and Technology (HKUST)</a>, where I work on efficient deep learning (algorithm-hardware co-design) and human-centric computer vision. I am a member of <a href="http://vsdl.ust.hk/index.html">Vision and System Design Lab </a>, advised by <a href="https://seng.ust.hk/about/people/faculty/tim-kwang-ting-cheng?id=326">Prof. Tim Kwang-Ting CHENG</a>. I work closely with <a href="https://scholar.google.com/citations?user=lA7ylt4AAAAJ&hl">Zechun Liu</a> from Meta Reality Lab. Currently, I am working as a research intern at <a href="https://research.snap.com/team/creative-vision.html">Snap Research Creative Vision Group</a>, supervised by <a href="https://alanspike.github.io/">Jian Ren</a> and <a href="https://scholar.google.com/citations?user=bZdVsMkAAAAJ&hl=en">Anil Kag</a>. I have also conducted a research internship at Microsoft Research Asia (MSRA). Previously, I work with <a href="http://zhiqiangshen.com/">Prof. Zhiqiang Shen</a> from MBZUAI, <a href="https://www.mvig.org/">Prof. Cewu Lu</a> from SJTU, and <a href="https://www.ee.ucla.edu/mani-srivastava/">Prof. Mani Srivastava</a> from UCLA. 
              </p>
              </p>
              <p style="text-align:center">
                <a href="files/CV_for_XijieHuang.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=nFW2mqwAAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/Owen_Huangxj">Twitter (X)</a> &nbsp/&nbsp
                <a href="https://www.zhihu.com/people/huang-xi-jie-17">Zhihu (Áü•‰πé)</a> &nbsp/&nbsp
                <a href="https://github.com/HuangOwen">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images_all/my_photo1.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images_all/my_photo1.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <strong>[2024-09]</strong> Two papers (CoT-Influx and RoLoRA) are accepted by EMNLP 2024
            </p>
            <p>
              <strong>[2024-08]</strong> One paper (ACS-QAT) is accepted by TMLR
            </p>
            <p>
              <strong>[2024-07]</strong> Start my intership at <a href="https://research.snap.com/">Snap Research Santa Monica</a>
            </p>
            <p>
              <strong>[2023-09]</strong> One paper (LLM-FP4) is accepted by EMNLP 2023
            </p>
            <p>
              <strong>[2023-05]</strong> Start my internship at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsoft Research Asia (MSRA)</a>
            </p>
            <p>
              <strong>[2022-05]</strong> One paper (SDQ) is accepted by ICML 2022
            </p>
            <p>
              <strong>[2021-01]</strong> One paper (TIN) is accepted by TPAMI
            </p>
            <p>
              <strong>[2020-08]</strong> Start my PhD at Hong Kong University of Science and Technology (HKUST)
            </p>
            <p>
              <strong>[2020-06]</strong> Graduate from Shanghai Jiao Tong University (SJTU)
            </p>
            <p>
              <strong>[2020-03]</strong> One paper (PaStaNet) is accepted by CVPR 2020
            </p>
            <p>
              <strong>[2019-06]</strong> Start my research internship at University of California, Los Angeles (UCLA)
            </p>
            <p>
              <strong>[2019-03]</strong> One paper (TIN) is accepted by CVPR 2019
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests lie in the general area of artificial intelligence, particularly in <b>efficient deep learning</b> and <b>human-centric computer vision</b>. More concretely, My research interests focus on quantization, algorithm-hardware co-design, human-object interation, and healthcare. Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/rolora.png', height="70" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>RoLoRA: Finetuning Outlier-free Model with Rotation for Weight-Activation Quantization</papertitle>
              <br>
              <b>Xijie Huang</b>, Zechun Liu, Shih-Yang Liu, Kwang-Ting Cheng
              <br>
              <em>EMNLP 2024 Findings</em>
              <br>
              <a href="https://arxiv.org/abs/2407.08044">Paper</a>/<a href="https://huggingface.co/collections/ScarletAce/rolora-66f5f228a90681c7c4512b28">Code</a>/<a href="https://github.com/HuangOwen/RoLoRA">HF Repo</a>
              <p></p>
              <p>A LoRA-based scheme for effective weight-activation quantization. RoLoRA utilizes rotation for outlier elimination and proposes rotation-aware fine-tuning to preserve the outlier-free characteristics in rotated LLMs.
              </p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/COT-MAX.png', height="80" width="190"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning</papertitle>
              <br>
              <b>Xijie Huang</b>, Li Lyna Zhang, Kwang-Ting Cheng, Fan Yang, Mao Yang
              <br>
              <em>EMNLP 2024 Main</em>
              <br>
              <a href="https://arxiv.org/abs/2312.08901">Paper</a>
              <p></p>
              <p>A novel approach to push the boundaries of few-shot CoT learning to improve LLM math reasoning capabilities. We propose a coarse-to-fine pruner as a plug-and-play module for LLMs, which first identifies crucial CoT examples from a large batch and then further prunes unimportant tokens. 
              </p>
            </td>
          </tr>

          <tr></tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/GQA-LUT.png', height="72" width="190"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers</papertitle>
              <br>
              Pingcheng Dong, Yonghao Tan, Dong Zhang, Tianwei Ni, Xuejiao Liu, Yu Liu, Peng Luo, Luhong Liang, Shih-Yang Liu, <b>Xijie Huang</b>, Huaiyu Zhu, Yun Pan, Fengwei An, Kwang-Ting Cheng
              <br>
              <em>ACM/IEEE Design Automation Conference (DAC) 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2403.19591">Paper</a>/<a href="https://github.com/PingchengDong/GQA-LUT">Code</a>
              <p></p>
              <p>A genetic LUT-Approximation algorithm namely GQA-LUT that can automatically determine the parameters with quantization awareness. The results demonstrate that GQA-LUT achieves negligible degradation on the challenging semantic segmentation task for both vanilla and linear Transformer models. Besides, proposed GQA-LUT enables the employment of INT8-based LUT-Approximation that achieves an area savings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the high-precision FP/INT 32 alternatives. 
              </p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/QAT-ACS.png', height="120" width="190"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Efficient Quantization-aware Training with Adaptive Coreset Selection</papertitle>
              <br>
              <b>Xijie Huang</b>, Zechun Liu, Shih-Yang Liu, Kwang-Ting Cheng
              <br>
              <em>Transactions on Machine Learning Research (TMLR) 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2306.07215">Paper</a>/<a href="https://github.com/HuangOwen/QAT-ACS">Code</a>/<a href="https://openreview.net/forum?id=4c2pZzG94y">OpenReview</a>
              <p></p>
              <p>A new angle through the coreset selection to improve the training efficiency of quantization-aware training. Our method can achieve an accuracy of 68.39% of 4-bit quantized ResNet-18 on the ImageNet-1K dataset with only a 10% subset, which has an absolute gain of 4.24% compared to the previous SoTA.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/EVQ.png', height="130" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Variation-aware Vision Transformer Quantization</papertitle>
              <br>
              <b>Xijie Huang</b>, Zhiqiang Shen, Kwang-Ting Cheng
              <br>
              <em>In Submission</em>
              <br>
              <a href="https://arxiv.org/abs/2307.00331">Paper</a>/<a href="https://github.com/HuangOwen/VVTQ">Code</a>
              <p></p>
              <p>An analysis of the underlying difficulty of ViT quantization in the view of variation. A multi-crop knowledge distillation-based quantization method is proposed.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/FPQ.png', height="100" width="190"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>LLM-FP4: 4-Bit Floating-Point Quantized Transformers</papertitle>
              <br>
              Shih-Yang Liu, Zechun Liu, <b>Xijie Huang</b>, Pingcheng Dong, Kwang-Ting Cheng
              <br>
              <em>EMNLP 2023 Main</em>
              <br>
              <a href="https://arxiv.org/abs/2310.16836">Paper</a>/<a href="https://github.com/nbasyl/LLM-FP4">Code</a>
              <p></p>
              <p>A per-channel activation quantization scheme with additional scaling factors that can be reparameterized as exponential biases of weights, incurring a negligible cost. Our method can quantize both weights and activations in the Bert model to only 4-bit and achieves an average GLUE score of 80.07, which is only 3.66 lower than the full-precision model, significantly outperforming the previous state-of-the-art method that had a gap of 11.48.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/TCAS.png', height="120" width="190"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>A Tiny Accelerator for Mixed-bit Sparse CNN based on Efficient Fetch Method of SIMO SPad</papertitle>
              <br>
              Xianghong Hu, Xuejiao Liu, Yu Liu, Haowei Zhang, <b>Xijie Huang</b>, Xihao Guan, Luhong Liang, Chi Ying Tsui, Xiaomeng Xiong, Kwang-Ting Cheng
              <br>
              <em>IEEE Transactions on Circuits and Systems II: Express Briefs (TCAS-II) 2023</em>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10071554">Paper</a>
              <p></p>
              <p>A tiny accelerator for mixed-bit sparse CNNs featuring a novel scheme of single vector-based compressed sparse filter (CSF) method and single input multiple output scratch pad (SIMO SPad) to effectively compress weight and fetch the needed input activation. 
              </p>
            </td>
          </tr>

          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/VideoPro.png', height="120" width="190"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>VideoPro: A Visual Analytics Approach for Interactive Video Programming</papertitle>
              <br>
              Jianben He, Xingbo Wang, Kam Kwai Wong, <b>Xijie Huang</b>, Changjian Chen, Zixin Chen, Fengjie Wang, Min Zhu, Huamin Qu
              <br>
              <em>IEEE Transactions on Visualization and Computer Graphics (VIS) 2023 </em>
              <br>
              <a href="https://arxiv.org/abs/2308.00401">Paper</a>
              <p></p>
              <p> A visual analytics approach to support flexible and scalable video data programming for model steering with reduced human effort.
              </p>
            </td>
          </tr>

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='loss_image'><img src='images/SDQ.png', height="140" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>SDQ: Stochastic Differentiable Quantization with Mixed Precision</papertitle>
              <br>
              <b>Xijie Huang</b>, Zhiqiang Shen, Shichao Li, Zechun Liu, Xianghong Hu, Jeffry Wicaksana, Eric Xing, Kwang-Ting Cheng
              <br>
              <em>ICML</em>, 2022 &nbsp <font color="red"><strong>(Spotlight)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2206.04459">Paper</a> /
              <a href="https://www.bilibili.com/video/BV1uY4y1E7Nk/?share_source=copy_web&vd_source=9b0b25da7812c022f1ee27fdc58aeb82">Talk</a> / 
              <a href="https://icml.cc/media/icml-2022/Slides/16258.pdf">Slides</a> 
              <p>A novel stochastic quantization framework to learn the optimal mixed precision quantization strategy. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/HOHC.png', height="140" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Automated Vision-Based Wellness Analysis for Elderly Care Centers</papertitle>
              <br>
              <b>Xijie Huang</b>, Jeffry Wicaksana, Shichao Li, Kwang-Ting Cheng
              <br>
              <em>AAAI Workshop on Health Intelligence, 2022</em>
              <br>
              <a href="https://arxiv.org/abs/2112.10381">Paper</a>
              <p></p>
              <p>An automatic, vision-based system for monitoring and analyzing the physical and mental well-being of senior citizens.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/FedMix.png', height="140" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>FedMix: Mixed Supervised Federated Learning for Medical Image Segmentation</papertitle>
              <br>
              Jeffry Wicaksana, Zengqiang Yan, Dong Zhang, <b>Xijie Huang</b>, Huimin Wu, Xin Yang, Kwang-Ting Cheng
              <br>
              <em> IEEE Transactions on Medical Imaging (TMI), 2022</em>
              <br>
              <a href="https://arxiv.org/abs/2205.01840">Paper</a>/<a href="https://github.com/Jwicaksana/FedMix">Code</a>
              <p></p>
              <p>A label-agnostic unified federated learning framework, named FedMix, for medical image segmentation based on mixed image labels. In FedMix, each client updates the federated model by integrating and effectively making use of all available labeled data ranging from strong pixel-level labels, weak bounding box labels, to weakest image-level class labels.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/TIN.png', height="140" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Transferable Interactiveness Prior for Human-Object Interaction Detection</papertitle>
              <br>
              Yong-Lu Li, Siyuan Zhou, <b>Xijie Huang</b>, Liang Xu, Ze Ma, Hao-Shu Fang, Cewu Lu
              <br>
              <em>TPAMI 2021/CVPR 2019</em>
              <br>
              <a href="https://arxiv.org/abs/2101.10292">Paper (TPAMI version)</a> /
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Transferable_Interactiveness_Knowledge_for_Human-Object_Interaction_Detection_CVPR_2019_paper">Paper (CVPR version)</a> /
              <a href="https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network">Code</a> 
              <p></p>
              <p>A transferable knowledge learner and can be cooperated with any HOI detection models to achieve desirable results. TIN outperforms state-of-the-art HOI detection results by a great margin, verifying its efficacy and flexibility.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/pasta.png', height="120" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>PaStaNet: Toward Human Activity Knowledge Engine</papertitle>
              <br>
              Yong-Lu Li, Liang Xu, Xinpeng Liu, <b>Xijie Huang</b>, Mingyang Chen, Shiyi Wang, Hao-Shu Fang, Cewu Lu
              <br>
              <em>CVPR 2020</em>
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Li_PaStaNet_Toward_Human_Activity_Knowledge_Engine_CVPR_2020_paper.html">Paper</a>/
              <a href="https://github.com/DirtyHarryLYL/HAKE-Action">Code</a>
              <p></p>
              <p>A large-scale knowledge base PaStaNet, which contains 7M+ PaSta annotations. And two corresponding models are proposed: first, we design a model named Activity2Vec to extract PaSta features, which aim to be general representations for various activities.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/finger.png', height="120" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Latent Fingerprint Image Enhancement based on progressive generative adversarial network</papertitle>
              <br>
              <b>Xijie Huang</b>, Peng Qian, Manhua Liu
              <br>
              <em>CVPRW 2020</em>
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/html/w48/Huang_Latent_Fingerprint_Image_Enhancement_Based_on_Progressive_Generative_Adversarial_Network_CVPRW_2020_paper.html">Paper</a> 
              <p></p>
              <p>A latent fingerprint enhancement method based on the progressive generative adversarial network (GAN).
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/HAKE.png', height="120" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>HAKE: Human Activity Knowledge Engine</papertitle>
              <br>
              Yong-Lu Li, Liang Xu, Xinpeng Liu, <b>Xijie Huang</b>, Ze Ma, Hao-Shu Fang, Cewu Lu
              <br>
              <em>Preprint</em>
              <br>
              <a href="https://arxiv.org/abs/1904.06539">Paper</a>/
              <a href="http://hake-mvig.cn/home/">Project webpage</a>
              <p></p>
              <p>A large-scale Human Activity Knowledge Engine (HAKE) based on the human body part states to promote the activity understanding.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mpi_image'><img src='images/trojan.png', height="120" width="180"></div>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>NeuronInspect: Detecting Trojan Backdoors in Deep Neural Networks via VisualInterpretability</papertitle>
              <br>
              <b>Xijie Huang</b>, Moustafa Alzantot, Mani.Srivastava
              <br>
              <em>Preprint</em>
              <br>
              <a href="https://arxiv.org/pdf/1911.07399.pdf">Paper</a>
              <p></p>
              <p>A framework to detect trojan backdoors in DNNs via output explanation techniques
              </p>
            </td>
          </tr>



        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody></tbody>
          <tr>
            <td>
              <heading>Experiences</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/ms.jpg", height="123" width="160", alt="cs188"></td>
            <td width="75%" valign="center">
              <strong>Microsoft Research Asia</strong>
              <br>
              May 2023 - Feb 2024
              <br>
              Research Intern in <a href="https://www.microsoft.com/en-us/research/group/systems-research-group-asia/">System Research Group (SRG)</a>
              <br>
              Mentor: <a href="https://www.microsoft.com/en-us/research/people/lzhani/">
                Li Lyna Zhang</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/snap.png" , height="83" width="160", alt="cs188">
            </td>
            <td width="75%" valign="center">
              <strong>Snap Research</strong>
              <br>
              July 2024 - 
              <br>
              Research Intern in <a href="https://research.snap.com/team/creative-vision.html/">Creative Vision group</a>
              <br>
              Mentor: <a href="https://alanspike.github.io/"> Jian Ren</a> and <a href="https://anilkagak2.github.io/">Anil Kag</a>


            </td>
          </tr>


				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              Reviewer, ICLR 2024, ACM MM 2024, EMNLP 2023-2024, NeurIPS 2023-2024, ICCV 2023, CVPR 2023, WACV 2022-2024, AAAI 2022-2025, ECCV 2022/2024, TNNLS, TMLR
              <br>
              <strong>Top 10% Reviewer</strong>, ICML 2022
              <br>
              Program Committee, <a href="https://sites.google.com/view/lbqnn-iccv-23/home">ICCV 2023 Workshop on Low-Bit Quantized Neural Networks </a>
              <br>
              Technical Program Committee, <a href="https://sites.google.com/view/rcv2023/organizing-committee">ICCV 2023 Workshop on Resource Efficient Deep Learning for Computer Vision</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TA.png" , height="123" width="160", alt="cs188">
            </td>
            <td width="75%" valign="center">
              COMP 2211 (Exploring Artificial Intelligence), Lecture: Professor Desmond Tsoi
              <br>
              COMP 5421 (Computer Vision), Lecture: Professor Dan Xu
              <br>
              COMP 1021 (Introduction to Computer Science), Lecturer: Professor David Rossitor
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/scholarship.png" , height="123" width="160", alt="cs188">
            </td>
            <td width="75%" valign="center">
              National Scholarship (Top 2% students in SJTU), 2017
              <br>
              A Class Scholarship (Top 2% students in SJTU), 2017
              <br>
              CSST Scholarship (USD $5,343 from UCLA), 2019
              <br>
              RongChang Academic Scholarship (Top 20 in SJTU), 2019
              <br>
              RedBird Scholarship, Postgraduate Studentship, 2020-2022
              <br>
              AAAI-22 Student Scholarship, 2022
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Thanks to <a href="https://github.com/jonbarron/jonbarron_website">Barron's website template </a>. 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
